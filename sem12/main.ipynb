{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzOavFVJteMB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Композиции классификаторов (градиентный бустинг)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCn8xDPhteMB",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!pip install catboost, lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3qWY0M5LA6r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2_VhyWeteMB",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from mpl_toolkits import mplot3d\n",
    "from copy import deepcopy\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "from sklearn import tree, base\n",
    "import itertools\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, BaggingClassifier)\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, RepeatedKFold\n",
    "from sklearn.datasets import make_classification, make_regression, load_wine, load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqhbF2bhteMB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ComBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Рассмотрим следующее ансамблирование ответов:\n",
    "$$\n",
    "b(x) = \\frac{1}{T}\\sum_{t=1}^{T} b_t(x)\n",
    "$$\n",
    "\n",
    "Рассмотрим отступ объектов:\n",
    "$$\n",
    "M(x) = \\Gamma_y\\bigr(x\\bigr) - \\max_{y} \\Gamma_y\\bigr(x\\bigr)\n",
    "$$\n",
    "\n",
    "Идея в том, что каждый $b_t$ компенсирует ошибки ансамбля, состоящего из всех предыдущих моделей:\n",
    "$$\n",
    "Q\\bigr(b_t, U_t\\bigr) = \\sum_{x \\in U_t}\\left[M(x) < 0\\right] \\to \\min_{b_t},\n",
    "$$\n",
    "$$\n",
    "U_t = \\left\\{x| M_l < M_{t-1}(x) < M_g\\right\\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beirFGPXteMB",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class ComBoost(object):\n",
    "    def __init__(self, base_estimator=None, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "        if base_estimator:\n",
    "            self.base_estimator = base_estimator\n",
    "        self.b = [base.clone(self.base_estimator) for _ in range(self.n_estimators)]\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {'n_estimators': self.n_estimators, \n",
    "                'base_estimator': self.base_estimator}\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_predict_proba(pred, b, b0):\n",
    "        new_pred = np.zeros((len(pred), len(b0.classes_)))\n",
    "        for i, cl in enumerate(b.classes_):\n",
    "            new_pred[:, cl] = pred[:, i]\n",
    "        return new_pred\n",
    "        \n",
    "    def fit(self, X, Y, l0=0, l1=100, l2=None, dl=100):\n",
    "        def margin(pr, y):\n",
    "            cop = pr.copy()\n",
    "            cop[y] = -1\n",
    "            return pr[y] - cop.max()\n",
    "        \n",
    "        if l2 is None:\n",
    "            l2 = len(X)\n",
    "        \n",
    "        for t, b in enumerate(self.b):\n",
    "            if t == 0:\n",
    "                b.fit(X, Y)\n",
    "                pred = b.predict_proba(X)\n",
    "                M = np.array([margin(pred[i], Y[i]) for i in range(len(Y))])\n",
    "            else:\n",
    "                indexes = sorted(np.arange(0, len(X)), key = lambda i: M[i])\n",
    "                X_new = X[indexes]\n",
    "                Y_new = Y[indexes]\n",
    "                dict_of_param = []\n",
    "                for k in range(l1, l2, dl):\n",
    "                    new_item = {'l0': l0, \n",
    "                                'k': k}\n",
    "                    \n",
    "                    local_b = base.clone(self.base_estimator)\n",
    "                    local_b.fit(X_new[l0:k], Y_new[l0:k])\n",
    "                    \n",
    "                    pred = self.fix_predict_proba(local_b.predict_proba(X), local_b, self.b[0])\n",
    "                    M_new = np.array([margin(pred[i], Y[i]) for i in range(len(Y))])\n",
    "                    \n",
    "                    new_item['Q'] = (M+M_new < 0).sum()\n",
    "                    dict_of_param.append(new_item)\n",
    "                    \n",
    "                element = sorted(dict_of_param, key=lambda x: x['Q'])[0]\n",
    "                b.fit(X_new[element['l0']:element['k']], \n",
    "                      Y_new[element['l0']:element['k']])\n",
    "                \n",
    "                pred = self.fix_predict_proba(b.predict_proba(X), local_b, self.b[0])\n",
    "                M = M + np.array([margin(pred[i], Y[i]) for i in range(len(Y))])\n",
    "                \n",
    "                    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return np.mean([self.fix_predict_proba(elem.predict_proba(X), elem, self.b[0]) for elem in self.b], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пример использования\n",
    "\n",
    "Данный метод позволяет строить ансамли для произвольных базовых функций. Далее приводится пример:\n",
    "- Решающего дерева\n",
    "- SVM\n",
    "- Логистической регресии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('SCORE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cT-L6N5OteMC",
    "outputId": "67246c12-b53d-4c77-e378-dd9d427056dc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "model = ComBoost(DecisionTreeClassifier(max_depth=2))\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "model = SVC(probability=True)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLmBqjS9teMC",
    "outputId": "25928568-1746-473e-c8f7-3dbf30b36b04",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "model = ComBoost(SVC(probability=True))\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "model = LogisticRegression()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjZQNVGIteMC",
    "outputId": "8a93a3fa-acef-441e-8628-efb4daf0b96c",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "model = ComBoost(LogisticRegression())\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Выводы\n",
    "Получаем, что ансамблирование не ухудшает качества на кроссвалидации. В случае простых моделей (дерево с ограниченной глубиной), качество улучшается значительно, а в случае сложны моделей (SVM) качество улучшается не значительно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmhySFdjteMC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Рассмотрим следующее ансамблирование ответов:\n",
    "$$\n",
    "b(x) = \\sum_{t=1}^{T} b_t(x)\n",
    "$$\n",
    "\n",
    "Рассмотрим среднеквадратичекое отклонение:\n",
    "$$\n",
    "L = \\sum_{i=1}^{l}\\left(b(x_i) - y_i\\right)^2\n",
    "$$\n",
    "\n",
    "Идея состоит в том, что каждая новая модель пытается аппроксимировать остатки которые оставили прошлые модели:\n",
    "$$\n",
    "L_t = \\sum_{i=1}^{l}\\left(b_t(x_i) - (y_i - \\sum_{j=1}^{t}b_j(x_i))\\right)^2 \\min_{b_t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLC8MVOuteMC",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class GradientBoostingRegression(object):\n",
    "    def __init__(self, base_estimator=None, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = DecisionTreeRegressor(max_depth=1)\n",
    "        if base_estimator:\n",
    "            self.base_estimator = base_estimator\n",
    "            \n",
    "        self.b = [base.clone(self.base_estimator) for _ in range(self.n_estimators)]\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {'n_estimators': self.n_estimators, \n",
    "                'base_estimator': self.base_estimator}\n",
    "        \n",
    "    def score(self, X, Y):\n",
    "        return ((self.predict(X) - Y)**2).mean()\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        residual = Y.copy()\n",
    "        for t, b in enumerate(self.b):\n",
    "            b.fit(X, residual)\n",
    "            residual = residual - b.predict(X)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return np.sum([elem.predict(X) for elem in self.b], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пример использования\n",
    "\n",
    "Данный метод позволяет строить ансамли для произвольных базовых функций. Далее приводится пример:\n",
    "- Решающего дерева\n",
    "- SVM\n",
    "- Линейная регресии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = DecisionTreeRegressor(max_depth=2)\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqEYhEkNteMC",
    "outputId": "1a9e14f0-5008-4556-d18e-1fe6f0463652",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = GradientBoostingRegression(DecisionTreeRegressor(max_depth=2))\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = SVR()\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93SBWXASteMD",
    "outputId": "991e234e-0c2b-438d-8fb2-522ef556d139",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = GradientBoostingRegression(SVR())\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = SVR(kernel='linear')\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PnBbGcxteMD",
    "outputId": "d186e207-7623-4587-f577-603dc636f969",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = GradientBoostingRegression(SVR(kernel='linear'))\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = LinearRegression()\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhYy17TTteMD",
    "outputId": "fe578618-a432-405f-e4df-2390eed887b2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = GradientBoostingRegression(LinearRegression())\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вывод\n",
    "\n",
    "Качество очень сильно зависит от выбранной базовой функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrFZTB31teMD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Пакет `xgboost` является один из библиотек для построения деревьев на основе градиентного бустинга. В основном все такие библиотеки работают только с решающими деревьями (ансамбли принято строить над деревьями), подробное описания модификации описано в [статье](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf).\n",
    "\n",
    "- Ускорение заключается в более оптимальном подборе порогов в каждой вершине дерева.\n",
    "- Модифицировали алгоритм для паралельного обучения деревьев (напомним, что классический бустинг не позволяет выполнять паралельную обработку).\n",
    "- Специальные инженерные трюки для сбалансированого использования кеша.\n",
    "\n",
    "Данный framework имеет следующие плюсы:\n",
    "- Хорошая документация.\n",
    "- Позволяет легко паралелить вычисления.\n",
    "- Легко использовать с `sklearn` и `numpy` (но с потерей производительности)\n",
    "\n",
    "Недостатки:\n",
    "- Нету поддержки GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4PnCcUkteMD",
    "outputId": "a36bf75e-1503-4242-f44a-e660b5c7a5cf",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', random_state=6)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFLWA6VBteMD",
    "outputId": "5bb80b10-1dde-44b2-a712-9bda076b4235",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=6)\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=6)\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Пакет `lightgbm` представлен Microsoft как реализация градиентного бустинга над деревьями. Работа с подробным описаниям введений описана [тут](https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf).\n",
    "\n",
    "Сам метод модифицирует метод XGboost с дополнительной процедурой ресемплинга объектов для ускорения построения деревьев.\n",
    "\n",
    "\n",
    "Данный фраймворк имеет следующие положительные моменты:\n",
    "- Хорошая документация.\n",
    "- Имеется поддержка GPU.\n",
    "- Имеет поддержку категориальных признаков на основе метода Фишера, который описан [тут](https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features).\n",
    "\n",
    "Недостатки:\n",
    "- Сложно использовать с `numpy` и `sklearn` так как требует специфичного формата данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "\n",
    "train_data = lgb.Dataset(X, label=y)\n",
    "param = {'num_leaves': 31, \n",
    "         'objective': 'multiclass', \n",
    "         'num_class': 2, \n",
    "         'metric': ['multi_logloss']}\n",
    "\n",
    "num_round = 10\n",
    "bst = lgb.train(param, train_data, num_boost_round=10)\n",
    "\n",
    "(bst.predict(X).argmax(axis=-1) == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QK6_SiPteMD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Пакет `catboost` представлен Яндексом для построения ансамблей моделей на базе решающих деревьев. Подробное описание доступно в [работе](https://papers.nips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf).\n",
    "\n",
    "- Основной идеей и посылом для создания CatBoost была разработка метода работы с категориальными признаками (отсюда и названия).\n",
    "- В классическом машинном обучении принято категориальные признаки кодировать One-Hot векторами. В работе предлагался метод, который выделяет кластеры внутри категориального признака на основе предлагаемых в работе статистик.\n",
    "\n",
    "\n",
    "Интересное замечание, что в статье показано, что данный метод работает лучше чем XGboost и LightGBM, но в реальных кейсах это не так...\n",
    "\n",
    "Данный framework имеет следующие плюсы:\n",
    "- Хорошая документация.\n",
    "- Позволяет легко паралелить вычисления на GPU.\n",
    "- Легко использовать с `sklearn` и `numpy` (но с потерей производительности).\n",
    "- Поддержка категориальных признаков (причем продвинутая, в отличии от простых методов Фишера).\n",
    "\n",
    "Недостатки:\n",
    "- Во многих задачах показывает себя хуже чем XGboost и LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H7GxrtHteMD",
    "outputId": "773cf684-cfc4-4f1c-8358-9724d5797491",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "\n",
    "model = CatBoostClassifier(iterations=2,\n",
    "                           depth=2,\n",
    "                           learning_rate=1,\n",
    "                           loss_function='Logloss',\n",
    "                           verbose=True, task_type='CPU')\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcYKogs2tgxE",
    "outputId": "8ab355d0-28d4-4dd7-e866-d79d0ed5e175",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "\n",
    "model = CatBoostClassifier(iterations=2,\n",
    "                           depth=2,\n",
    "                           learning_rate=1,\n",
    "                           loss_function='Logloss',\n",
    "                           verbose=True, task_type='GPU')\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "print('SCORE: %.2f (%.2f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
